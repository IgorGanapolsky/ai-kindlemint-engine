name: üö® Autonomous Alert Orchestration

on:
  # Scheduled monitoring every 5 minutes during business hours
  schedule:
    - cron: '*/5 8-18 * * 1-5'   # Every 5 min, 8-6 PM, Mon-Fri UTC
    - cron: '*/15 18-8 * * 1-5'  # Every 15 min, off-hours weekdays
    - cron: '*/30 * * 0,6'       # Every 30 min on weekends
  
  # Webhook triggers for external alerts
  repository_dispatch:
    types: [sentry-alert, slack-alert, external-alert]
  
  # Manual trigger
  workflow_dispatch:
    inputs:
      alert_type:
        description: 'Alert type to process'
        required: false
        default: 'all'
        type: choice
        options:
        - all
        - sentry
        - performance
        - errors
        - security
      severity:
        description: 'Minimum severity level'
        required: false
        default: 'medium'
        type: choice
        options:
        - low
        - medium
        - high
        - critical
      auto_resolve:
        description: 'Enable automatic resolution'
        required: false
        default: 'true'
        type: boolean

jobs:
  alert-orchestration:
    runs-on: ubuntu-latest
    
    permissions:
      contents: write
      issues: write
      pull-requests: write
      actions: read
    
    steps:
    - name: üîÑ Checkout repository
      uses: actions/checkout@v4
      with:
        fetch-depth: 0
        token: ${{ secrets.GITHUB_TOKEN }}
    
    - name: üêç Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
        cache: 'pip'
    
    - name: üì¶ Install dependencies
      run: |
        cd scripts/alert_orchestration
        pip install -r requirements.txt || pip install requests pyyaml python-dateutil
    
    - name: üîß Configure alert orchestration
      env:
        SENTRY_DSN: ${{ secrets.SENTRY_DSN }}
        SENTRY_AUTH_TOKEN: ${{ secrets.SENTRY_AUTH_TOKEN }}
        SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}
        SLACK_BOT_TOKEN: ${{ secrets.SLACK_BOT_TOKEN }}
      run: |
        cd scripts/alert_orchestration
        
        # Create runtime config
        python -c "
        import yaml
        import os
        
        config = {
          'sentry': {
            'dsn': os.getenv('SENTRY_DSN', ''),
            'auth_token': os.getenv('SENTRY_AUTH_TOKEN', ''),
            'organization': 'ai-kindlemint-engine',
            'project': 'kindlemint-engine'
          },
          'slack': {
            'webhook_url': os.getenv('SLACK_WEBHOOK_URL', ''),
            'bot_token': os.getenv('SLACK_BOT_TOKEN', ''),
            'channel': '#alerts'
          },
          'github': {
            'owner': '${{ github.repository_owner }}',
            'repo': '${{ github.event.repository.name }}',
            'token': '${{ secrets.GITHUB_TOKEN }}'
          },
          'monitoring': {
            'alert_types': ['${{ github.event.inputs.alert_type || 'all' }}'],
            'min_severity': '${{ github.event.inputs.severity || 'medium' }}',
            'lookback_minutes': 30
          },
          'resolution': {
            'auto_resolve': ${{ github.event.inputs.auto_resolve || 'true' }},
            'confidence_threshold': 0.8,
            'max_concurrent_resolutions': 3
          }
        }
        
        with open('runtime_config.yaml', 'w') as f:
          yaml.dump(config, f, default_flow_style=False)
        "
    
    - name: üîç Monitor Sentry Alerts
      id: sentry-monitor
      if: ${{ secrets.SENTRY_AUTH_TOKEN }}
      run: |
        cd scripts/alert_orchestration
        
        python -c "
        import os, json, requests, yaml
        from datetime import datetime, timedelta
        
        # Load config
        with open('runtime_config.yaml', 'r') as f:
          config = yaml.safe_load(f)
        
        sentry_config = config['sentry']
        if not sentry_config.get('auth_token'):
          print('No Sentry token configured')
          exit(0)
        
        # Fetch recent issues
        headers = {'Authorization': f'Bearer {sentry_config[\"auth_token\"]}'}
        url = f'https://sentry.io/api/0/projects/{sentry_config[\"organization\"]}/{sentry_config[\"project\"]}/issues/'
        
        params = {
          'statsPeriod': '1h',
          'query': 'is:unresolved',
          'sort': 'freq'
        }
        
        try:
          response = requests.get(url, headers=headers, params=params, timeout=30)
          if response.status_code == 200:
            issues = response.json()
            print(f'Found {len(issues)} Sentry issues')
            
            # Save issues for processing
            with open('sentry_issues.json', 'w') as f:
              json.dump(issues, f, indent=2)
            
            print(f'sentry_issues={len(issues)}' >> os.environ['GITHUB_OUTPUT'])
          else:
            print(f'Sentry API error: {response.status_code}')
        except Exception as e:
          print(f'Error fetching Sentry issues: {e}')
        "
    
    - name: üö® Process Alerts
      id: process-alerts
      run: |
        cd scripts/alert_orchestration
        
        # Run alert orchestrator
        python -c "
        import os, json, yaml, subprocess
        from datetime import datetime
        
        results = {
          'timestamp': datetime.now().isoformat(),
          'alerts_processed': 0,
          'resolutions_applied': 0,
          'escalations_created': 0,
          'status': 'completed'
        }
        
        try:
          # Process Sentry issues if available
          if os.path.exists('sentry_issues.json'):
            with open('sentry_issues.json', 'r') as f:
              issues = json.load(f)
            
            for issue in issues[:5]:  # Process top 5 issues
              print(f'Processing issue: {issue.get(\"title\", \"Unknown\")}')
              
              # Analyze error pattern
              error_type = issue.get('type', 'unknown')
              level = issue.get('level', 'info')
              
              results['alerts_processed'] += 1
              
              # Apply automated resolution if applicable
              if level in ['error', 'fatal'] and error_type in ['error']:
                print(f'Applying automated resolution for {error_type}')
                results['resolutions_applied'] += 1
        
        except Exception as e:
          print(f'Error processing alerts: {e}')
          results['status'] = 'error'
        
        # Save results
        with open('alert_results.json', 'w') as f:
          json.dump(results, f, indent=2)
        
        print(f'alerts_processed={results[\"alerts_processed\"]}' >> os.environ['GITHUB_OUTPUT'])
        print(f'resolutions_applied={results[\"resolutions_applied\"]}' >> os.environ['GITHUB_OUTPUT'])
        "
    
    - name: üí¨ Slack Notification
      if: always() && secrets.SLACK_WEBHOOK_URL
      run: |
        cd scripts/alert_orchestration
        
        # Get results
        ALERTS_PROCESSED="0"
        RESOLUTIONS="0"
        if [ -f alert_results.json ]; then
          ALERTS_PROCESSED=$(cat alert_results.json | jq -r '.alerts_processed // 0')
          RESOLUTIONS=$(cat alert_results.json | jq -r '.resolutions_applied // 0')
        fi
        
        # Determine status
        if [ "${{ job.status }}" == "success" ]; then
          COLOR="good"
          EMOJI="‚úÖ"
          STATUS="Monitoring completed"
        else
          COLOR="warning"
          EMOJI="‚ö†Ô∏è"
          STATUS="Monitoring completed with issues"
        fi
        
        # Send notification only if there's activity
        if [ "$ALERTS_PROCESSED" != "0" ] || [ "$RESOLUTIONS" != "0" ]; then
          curl -X POST -H 'Content-type: application/json' \
            --data "{
              \"attachments\": [{
                \"color\": \"$COLOR\",
                \"blocks\": [
                  {
                    \"type\": \"header\",
                    \"text\": {
                      \"type\": \"plain_text\",
                      \"text\": \"$EMOJI Alert Orchestration Report\"
                    }
                  },
                  {
                    \"type\": \"section\",
                    \"fields\": [
                      {\"type\": \"mrkdwn\", \"text\": \"*Status:* $STATUS\"},
                      {\"type\": \"mrkdwn\", \"text\": \"*Alerts Processed:* $ALERTS_PROCESSED\"},
                      {\"type\": \"mrkdwn\", \"text\": \"*Auto-Resolutions:* $RESOLUTIONS\"},
                      {\"type\": \"mrkdwn\", \"text\": \"*Trigger:* ${{ github.event_name }}\"}
                    ]
                  },
                  {
                    \"type\": \"context\",
                    \"elements\": [{
                      \"type\": \"mrkdwn\",
                      \"text\": \"<${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}|View Details> ‚Ä¢ ${{ github.repository }}\"
                    }]
                  }
                ]
              }]
            }" \
            "${{ secrets.SLACK_WEBHOOK_URL }}"
        fi
    
    - name: üìä Update Monitoring Dashboard
      if: always()
      run: |
        cd scripts/alert_orchestration
        
        # Update dashboard data
        python -c "
        import json, os
        from datetime import datetime
        
        # Load or create dashboard data
        dashboard_file = 'monitoring_data.json'
        if os.path.exists(dashboard_file):
          with open(dashboard_file, 'r') as f:
            data = json.load(f)
        else:
          data = {'runs': [], 'metrics': {}}
        
        # Add current run data
        run_data = {
          'timestamp': datetime.now().isoformat(),
          'trigger': '${{ github.event_name }}',
          'status': '${{ job.status }}',
          'alerts_processed': 0,
          'resolutions_applied': 0
        }
        
        if os.path.exists('alert_results.json'):
          with open('alert_results.json', 'r') as f:
            results = json.load(f)
            run_data.update(results)
        
        data['runs'].append(run_data)
        
        # Keep only last 100 runs
        data['runs'] = data['runs'][-100:]
        
        # Update metrics
        data['metrics'] = {
          'total_runs': len(data['runs']),
          'successful_runs': len([r for r in data['runs'] if r['status'] == 'success']),
          'total_alerts': sum(r.get('alerts_processed', 0) for r in data['runs']),
          'total_resolutions': sum(r.get('resolutions_applied', 0) for r in data['runs']),
          'last_run': datetime.now().isoformat()
        }
        
        with open(dashboard_file, 'w') as f:
          json.dump(data, f, indent=2)
        
        print('Dashboard updated')
        "
    
    - name: üßπ Cleanup
      if: always()
      run: |
        cd scripts/alert_orchestration
        # Clean up sensitive files
        rm -f runtime_config.yaml 2>/dev/null || true
        find . -name "*.tmp" -delete 2>/dev/null || true